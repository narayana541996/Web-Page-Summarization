{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ggh5oAQwlmc",
        "outputId": "2e28276f-adb9-4ccb-9dc3-7dc31ba6a7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "# !pip install huggingface\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "# !pip install keytotext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from transformers import pipeline\n",
        "# from transformers import GPT2Tokenizer, GPT2Model\n",
        "# from transformers import BertTokenizer, BertGenerationEncoder, BertModel\n",
        "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "# from keytotext import pipeline, trainer, make_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AMnlalpVwwKw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/t5-model/t5-base-config.json', return_dict = True)\n",
        "def generate(text, model, tokenizer):\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(f'WebNLG: {text} <\\s>')\n",
        "  outputs = model.generate(input_ids)\n",
        "  return tokenizer.decode(outputs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "A4NA0d8SbbM3",
        "outputId": "3d9e5cf3-7f7e-43ca-eb78-733b161b34a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    411\u001b[0m                         \u001b[0;34mf\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to locate the file /content/drive/MyDrive/t5-model/t5-base-config.json which is necessary to load this pretrained model. Make sure you have saved the model properly.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b8383c2b1587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/t5-model/t5-base-config.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'WebNLG: {text} <\\s>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    413\u001b[0m                     ) from e\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/content/drive/MyDrive/t5-model/t5-base-config.json' at '/content/drive/MyDrive/t5-model/t5-base-config.json'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Lakshmi | play | cricket')"
      ],
      "metadata": {
        "id": "zzSUDWoKGc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # text = 'One block has a search bar to search flights and two buttons to select to and fro dates. It has one more button to search. Another block has check buttons to select flexibility in dates and flexibility in airport. Then there is a third block with the search results showing a list of flights.'\n",
        "# text = ['1 search bar', '3 buttons', '2 checkbuttons', '1 search results']\n",
        "# text2 = ['1 search bar','airports','top']\n",
        "# text3 = '1 search bar | airports | top'\n",
        "# # key to text\n",
        "# # pipe = pipeline('k2t-base')\n",
        "# # print(pipe(text2))\n",
        "# # -------------------------\n",
        "# # train_df = make_dataset('common_gen', split = 'train')\n",
        "# # test_df = make_dataset('common_gen', split = 'test')\n",
        "# # model = trainer()\n",
        "# # model.from_pretrained(model_name = 't5-small')\n",
        "# # model.train(train_df = train_df, test_df = test_df)\n",
        "# # model.upload(hf_username = 'gagan3012', model_name = 'k2t-test3')\n",
        "# # ---------------------------\n",
        "\n",
        "# # with t5\n",
        "\n",
        "# # tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "# # input_ids = tokenizer.encode(text3, return_tensors = 'pt')\n",
        "# # model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict = True)\n",
        "# # output = model.generate(input_ids)\n",
        "# # tokenizer.decode(output[0])\n",
        "# # # ----------------------------------------\n",
        "\n",
        "\n",
        "# # train_df=pd.read_csv(‘webNLG2020_train.csv’, index_col=[0])\n",
        "# # train_df=train_df.iloc[  :35000,:]\n",
        "# # train_df=train_df.sample(frac = 1)\n",
        "# # batch_size=8\n",
        "# # num_of_batches=len(train_df)/batch_size\n",
        "\n",
        "# # optimizer = Adafactor(model.parameters(),lr=1e-3,\n",
        "# #                       eps=(1e-30, 1e-3),\n",
        "# #                       clip_threshold=1.0,\n",
        "# #                       decay_rate=-0.8,\n",
        "# #                       beta1=None,\n",
        "# #                       weight_decay=0.0,\n",
        "# #                       relative_step=False,\n",
        "# #                       scale_parameter=False,\n",
        "# #                       warmup_init=False)\n",
        "\n",
        "# # from IPython.display import HTML, display\n",
        "# # def progress(loss,value, max=100):\n",
        "# #  return HTML(\"\"\" Batch loss :{loss}\n",
        "# #       <progress    \n",
        "# # value='{value}'max='{max}',style='width: 100%'>{value}\n",
        "# #       </progress>\n",
        "# #              \"\"\".format(loss=loss,value=value, max=max))\n",
        " \n",
        "# # model.train()\n",
        "# # num_of_epochs = 4\n",
        "# # loss_per_10_steps=[]\n",
        "# # for epoch in range(1,num_of_epochs+1):\n",
        "# #   print('Running epoch: {}'.format(epoch))\n",
        "  \n",
        "# #   running_loss=0\n",
        "\n",
        "# #   out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "# #   for i in range(num_of_batches):\n",
        "# #     inputbatch=[]\n",
        "# #     labelbatch=[]\n",
        "# #     new_df=train_df[i*batch_size:i*batch_size+batch_size]\n",
        "# #     for indx,row in new_df.iterrows():\n",
        "# #       input = 'WebNLG: '+row['input_text']+'</s>' \n",
        "# #       labels = row['target_text']+'</s>'   \n",
        "# #       inputbatch.append(input)\n",
        "# #       labelbatch.append(labels)\n",
        "# #     inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
        "# #     labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
        "\n",
        "# #     # clear out the gradients of all Variables \n",
        "# #     optimizer.zero_grad()\n",
        "\n",
        "# #     # Forward propogation\n",
        "# #     outputs = model(input_ids=inputbatch, labels=labelbatch)\n",
        "# #     loss = outputs.loss\n",
        "# #     loss_num=loss.item()\n",
        "# #     logits = outputs.logits\n",
        "# #     running_loss+=loss_num\n",
        "# #     if i%10 ==0:      \n",
        "# #       loss_per_10_steps.append(loss_num)\n",
        "# #     out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "# #     # calculating the gradients\n",
        "# #     loss.backward()\n",
        "\n",
        "# #     #updating the params\n",
        "# #     optimizer.step()\n",
        "    \n",
        "# #   running_loss=running_loss/int(num_of_batches)\n",
        "# #   print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "\n",
        "# # # ----------------------------------------\n",
        "# # def generate(text, model = model, tokenizer = tokenizer):\n",
        "# #   model.eval()\n",
        "# #   input_ids = tokenizer.encode(text, return_tensors = 'pt')\n",
        "# #   outputs = model.generate(input_ids)\n",
        "# #   return tokenizer.decode(outputs[0])\n",
        "# # generate(text2)\n",
        "\n",
        "# # keytotext - huggingface\n",
        "# # tokenizer = AutoTokenizer.from_pretrained('gagan3012/keytotext-small')\n",
        "# # input_ids = tokenizer.encode(text3, return_tensors = 'pt')\n",
        "# # model = AutoModelWithLMHead.from_pretrained('gagan3012/keytotext-small')\n",
        "# # model(input_ids, decoder_input_ids = input_ids)\n",
        "\n",
        "# # model = pipeline(task = 'summarization', model = 'bert-base-uncased')\n",
        "# # model = pipeline(task = 'text-generation', model = 'gpt2')\n",
        "# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# # encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "# # encoder(tokenizer(text))\n",
        "# # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# # encoded_input = tokenizer(text)\n",
        "# # model = GPT2Model.from_pretrained('gpt2')\n",
        "# # output = model(encoded_input)\n",
        "# # output"
      ],
      "metadata": {
        "id": "1714fhEa67lT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}